import streamlit as st
import os
import tempfile
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter  
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.memory import ConversationBufferMemory
from google.oauth2 import service_account
from googleapiclient.discovery import build

# Gemini API í‚¤ ì„¤ì •
os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]

# ì„ë² ë”© ëª¨ë¸ ìºì‹±
@st.cache_resource
def get_embeddings():
    return GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=st.secrets["GOOGLE_API_KEY"]
    )

# Google Drive API ì´ˆê¸°í™” (ê°œì„ ëœ ë²„ì „)
@st.cache_resource
def init_drive_service():
    try:
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦ ì •ë³´ í™•ì¸
        if "google_credentials" not in st.secrets:
            st.error("Google ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        credentials = service_account.Credentials.from_service_account_info(
            st.secrets["google_credentials"],
            scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        
        service = build('drive', 'v3', credentials=credentials, cache_discovery=False)
        return service
        
    except Exception as e:
        st.error(f"Drive ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return None

# PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_pdf_files(service, folder_id):
    try:
        results = service.files().list(
            q=f"'{folder_id}' in parents and mimeType='application/pdf'",
            fields="files(id, name)"
        ).execute()
        return results.get('files', [])
    except Exception as e:
        st.error(f"Google Drive API ì˜¤ë¥˜: {str(e)}")
        return []

# PDF ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
def process_pdf(pdf, service):
    try:
        request = service.files().get_media(fileId=pdf['id'])
        file_content = request.execute()
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            temp_file.write(file_content)
            pdf_path = temp_file.name
        
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        
        for doc in documents:
            doc.metadata['source'] = pdf['name']
        
        os.unlink(pdf_path)
        return documents
    except Exception as e:
        st.warning(f"âš ï¸ {pdf['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

# ë²¡í„° ì €ì¥ì†Œ ìƒì„± í•¨ìˆ˜ ì¶”ê°€
def create_vector_store(texts, embeddings):
    try:
        return FAISS.from_documents(texts, embeddings)
    except Exception as e:
        st.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def main():
    st.title("ğŸ“„ IPRì‹¤ ë§¤ë‰´ì–¼ AI ì±—ë´‡")
    st.write("â˜† ìë£Œ ìˆ˜ì • ë˜ëŠ” ì¶”ê°€ í¬ë§ì‹œ ì£¼ì˜ ì—°êµ¬ì› ì—°ë½ â˜†")

    try:
        # Initialize services
        service = init_drive_service()
        embeddings = get_embeddings()
        
        if not service or not embeddings:
            st.error("í•„ìˆ˜ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return

        folder_id = st.secrets.get("FOLDER_ID")
        if not folder_id:
            st.error("í´ë” IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        pdf_files = get_pdf_files(service, folder_id)
        if not pdf_files:
            st.warning("ğŸ“‚ ë§¤ë‰´ì–¼ í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì»¨í…Œì´ë„ˆ êµ¬ì¡° ì •ì˜
        status_container = st.container()
        chat_container = st.container()
        
        # ë¶„ì„ ìƒíƒœ í™•ì¸
        if "analysis_completed" not in st.session_state:
            st.session_state.analysis_completed = False
            
            with status_container:
                status_placeholder = st.empty()
                
                # Process PDFs with memory management
                all_texts = []
                total_files = len(pdf_files)
                
                for idx, pdf in enumerate(pdf_files, 1):
                    status_placeholder.info(f"ğŸ“„ ë§¤ë‰´ì–¼ ë¶„ì„ ì¤‘... ({idx}/{total_files})\n\ní˜„ì¬ ì²˜ë¦¬ ì¤‘: {pdf['name']}")
                    documents = process_pdf(pdf, service)
                    all_texts.extend(documents)
                
                # Text splitting
                status_placeholder.info("ğŸ“„ í…ìŠ¤íŠ¸ ë¶„í•  ì‘ì—… ì§„í–‰ ì¤‘...")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=100,
                    length_function=len,
                    separators=["\n\n", "\n", " ", ""]
                )
                split_texts = text_splitter.split_documents(all_texts)
                
                # Create vector store
                status_placeholder.info("ğŸ“„ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
                vector_store = create_vector_store(split_texts, embeddings)
                
                if not vector_store:
                    st.error("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨")
                    return
                
                st.session_state.vector_store = vector_store
                st.session_state.analysis_completed = True

        # Show completion message
        with status_container:
            if st.session_state.analysis_completed:
                st.success("âœ… ë§¤ë‰´ì–¼ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!")

        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        with chat_container:
            if st.session_state.analysis_completed:
                # Chat interface setup
                retriever = st.session_state.vector_store.as_retriever(search_kwargs={"k": 3})
                
                system_template = """
                You are an expert AI assistant for IPR manuals. Base your answers strictly on the provided context.

                Guidelines:
                1. ALWAYS answer in Korean
                2. Use Markdown format
                3. Keep responses concise (2-4 sentences)
                4. If unsure, say "í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
                5. Cite source documents when possible
                Context:
                ----------------
                {context}
                """
                
                messages = [
                    SystemMessagePromptTemplate.from_template(system_template),
                    HumanMessagePromptTemplate.from_template("{question}")
                ]
                prompt = ChatPromptTemplate.from_messages(messages)

                # Initialize memory if not exists
                if "memory" not in st.session_state:
                    st.session_state.memory = ConversationBufferMemory(
                        memory_key="chat_history",
                        return_messages=True,
                        output_key="answer"
                    )

                # Initialize LLM and chain
                llm = ChatGoogleGenerativeAI(
                    model="gemini-2.0-flash",
                    temperature=0.7,
                    max_output_tokens=2048,
                )

                chain = ConversationalRetrievalChain.from_llm(
                    llm=llm,
                    retriever=retriever,
                    memory=st.session_state.memory,
                    combine_docs_chain_kwargs={'prompt': prompt},
                    return_source_documents=True
                )

                # Initialize chat history
                if "messages" not in st.session_state:
                    st.session_state.messages = []

                # Handle new messages
                if prompt := st.chat_input("ğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    
                    with st.spinner("ğŸ¤– ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        response = chain({"question": prompt})
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": response['answer']
                        })

                # Display chat history in reverse order (newest first)
                for message in reversed(st.session_state.messages):
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])
                        if message["role"] == "assistant" and message == st.session_state.messages[-1]:
                            # ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ (ë§ˆì§€ë§‰ ë©”ì‹œì§€ì¸ ê²½ìš°ì—ë§Œ)
                            sources = set([doc.metadata['source'] for doc in response['source_documents']])
                            if sources:
                                st.markdown("---")
                                st.markdown("**ì°¸ê³ í•œ ë¬¸ì„œ:**")
                                for source in sources:
                                    st.markdown(f"- {source}")

    except Exception as e:
        st.error(f"ğŸš¨ ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
if __name__ == "__main__":
    main()
