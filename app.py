import streamlit as st
import os
import tempfile
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.memory import ConversationBufferMemory
from google.oauth2 import service_account
from googleapiclient.discovery import build

# ğŸ” API ì„¤ì •
os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]

@st.cache_resource
def get_embeddings():
    return GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=st.secrets["GOOGLE_API_KEY"]
    )

@st.cache_resource
def init_drive_service():
    try:
        credentials = service_account.Credentials.from_service_account_info(
            st.secrets["google_credentials"],
            scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        return build('drive', 'v3', credentials=credentials, cache_discovery=False)
    except Exception as e:
        st.error(f"Drive ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return None

def get_pdf_files(service, folder_id):
    try:
        results = service.files().list(
            q=f"'{folder_id}' in parents and mimeType='application/pdf'",
            fields="files(id, name)"
        ).execute()
        return results.get('files', [])
    except Exception as e:
        st.error(f"Google Drive API ì˜¤ë¥˜: {str(e)}")
        return []

def process_pdf(pdf, service):
    try:
        request = service.files().get_media(fileId=pdf['id'])
        file_content = request.execute()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            temp_file.write(file_content)
            pdf_path = temp_file.name

        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        for i, doc in enumerate(documents):
            doc.metadata["source"] = pdf["name"]
            doc.metadata["page"] = doc.metadata.get("page", i)

        os.unlink(pdf_path)
        return documents
    except Exception as e:
        st.warning(f"âš ï¸ {pdf['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

def create_vector_store(texts, embeddings):
    try:
        return FAISS.from_documents(texts, embeddings)
    except Exception as e:
        st.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def main():
    st.set_page_config(page_title="ë³´ìœ  ê¸°ìˆ  ì±—ë´‡", layout="wide")
    st.title("ğŸ’¡ ìš°ë¦¬ íšŒì‚¬ ë³´ìœ  ê¸°ìˆ  ì•ˆë‚´ ì±—ë´‡")
    st.write("ê¶ê¸ˆí•œ ê¸°ìˆ  ë¶„ì•¼ë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ëœ ë³´ìœ  ê¸°ìˆ  ìë£Œë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.")

    try:
        service = init_drive_service()
        embeddings = get_embeddings()

        if not service or not embeddings:
            st.stop()

        folder_id = st.secrets.get("FOLDER_ID")
        if not folder_id:
            st.error("ğŸ“‚ í´ë” IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        pdf_files = get_pdf_files(service, folder_id)
        if not pdf_files:
            st.warning("ğŸ“‚ PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if "analysis_completed" not in st.session_state:
            st.session_state.analysis_completed = False
            status_placeholder = st.empty()
            all_texts = []

            for idx, pdf in enumerate(pdf_files, 1):
                status_placeholder.info(f"ğŸ“„ ë¬¸ì„œ ë¶„ì„ ì¤‘ ({idx}/{len(pdf_files)}): {pdf['name']}")
                documents = process_pdf(pdf, service)
                all_texts.extend(documents)

            status_placeholder.info("ğŸ§  ë¬¸ì„œ ë¶„í•  ì¤‘...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100,
                length_function=len,
            )
            split_texts = text_splitter.split_documents(all_texts)

            status_placeholder.info("ğŸ§  ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...")
            vector_store = create_vector_store(split_texts, embeddings)

            if not vector_store:
                return

            st.session_state.vector_store = vector_store
            st.session_state.all_pdfs = pdf_files
            st.session_state.analysis_completed = True

        if st.session_state.analysis_completed:
            st.success("âœ… ê¸°ìˆ  ìë£Œ ë¶„ì„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

            retriever = st.session_state.vector_store.as_retriever(search_kwargs={"k": 10})

            system_template = """
            ë„ˆëŠ” ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ  ì†Œê°œ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ìê°€ ì–´ë–¤ ê¸°ìˆ  ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆëŠ”ì§€ë¥¼ íŒŒì•…í•œ í›„,
            ê·¸ì™€ ê´€ë ¨ëœ **ìš°ë¦¬ íšŒì‚¬ê°€ ë³´ìœ í•œ ê¸°ìˆ **ì„ ìµœì†Œ 5ê±´ ì´ìƒ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

            ë‹µë³€ ì¡°ê±´:
            1. ì •ì˜ ì„¤ëª…ì€ ìƒëµí•˜ê³ , ê´€ë ¨ ê¸°ìˆ ì´ ìˆë‹¤ë©´ ë°”ë¡œ ê¸°ìˆ ëª…ì„ ì œì‹œí•©ë‹ˆë‹¤.
            2. ê° ê¸°ìˆ ì€ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
               - ê¸°ìˆ  ì„¤ëª… ìš”ì•½ (ê°„ë‹¨í•˜ê²Œ)
               - ë¬¸ì„œ ì´ë¦„
               - ë¬¸ì„œ ë‚´ í˜ì´ì§€ ë²ˆí˜¸
            3. ê´€ë ¨ ê¸°ìˆ ì´ ëª…í™•í•˜ì§€ ì•Šì•„ë„, ìœ ì‚¬ ê¸°ìˆ ì„ **ìµœì†Œ 5ê±´ ì´ìƒ** ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤.
            4. ë‹µë³€ì€ ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ê³ , Markdown í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
            5. ì¶œì²˜ ë¬¸ì„œ ì™¸ì˜ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ê³ , ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤.

            ê¸°ìˆ ìë£Œ:
            ----------------
            {context}
            """

            messages = [
                SystemMessagePromptTemplate.from_template(system_template),
                HumanMessagePromptTemplate.from_template("{question}")
            ]
            prompt = ChatPromptTemplate.from_messages(messages)

            if "memory" not in st.session_state:
                st.session_state.memory = ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True,
                    output_key="answer"
                )

            # âœ… ëª¨ë¸ ì˜¤ë¥˜ í•´ê²°: chat-bison-001 ì‚¬ìš©
            llm = ChatGoogleGenerativeAI(
                model="models/chat-bison-001",
                temperature=0.7,
                max_output_tokens=2048,
            )

            chain = ConversationalRetrievalChain.from_llm(
                llm=llm,
                retriever=retriever,
                memory=st.session_state.memory,
                combine_docs_chain_kwargs={"prompt": prompt},
                return_source_documents=True
            )

            if "messages" not in st.session_state:
                st.session_state.messages = []

            if prompt := st.chat_input("ê´€ì‹¬ ìˆëŠ” ê¸°ìˆ  í‚¤ì›Œë“œ ë˜ëŠ” ë¶„ì•¼ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
                st.session_state.messages.append({"role": "user", "content": prompt})

                with st.spinner("ğŸ¤– ê¸°ìˆ  ìë£Œ ê²€ìƒ‰ ì¤‘..."):
                    response = chain({"question": prompt})
                    answer = response["answer"]

                    source_docs = response.get("source_documents", [])
                    sources = set()
                    for doc in source_docs:
                        filename = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ")
                        page = doc.metadata.get("page", "ì•Œ ìˆ˜ ì—†ëŠ” í˜ì´ì§€")
                        page_display = int(page) + 1 if isinstance(page, int) else page
                        sources.add(f"- ğŸ“„ `{filename}`, **í˜ì´ì§€ {page_display}**")

                    if sources:
                        answer += "\n\n---\nğŸ“‘ **ì°¸ê³  ë¬¸ì„œ ìœ„ì¹˜:**\n" + "\n".join(sources)

                    if len(source_docs) < 5:
                        st.warning("ğŸ“Œ ê´€ë ¨ì„±ì´ ë‚®ì€ ê¸°ìˆ ë„ í¬í•¨í•˜ì—¬ ìµœì†Œ 5ê±´ ì œì‹œí–ˆìŠµë‹ˆë‹¤.")

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer
                    })

            for i in range(len(st.session_state.messages) - 1, -1, -2):
                if i > 0 and st.session_state.messages[i - 1]["role"] == "user":
                    st.markdown(f"**ğŸ™‹ ì§ˆë¬¸:** {st.session_state.messages[i - 1]['content']}")
                st.markdown(f"**ğŸ¤– ë‹µë³€:** {st.session_state.messages[i]['content']}")

            with st.expander("ğŸ“‚ PDF ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                for pdf in st.session_state.all_pdfs:
                    file_id = pdf["id"]
                    file_name = pdf["name"]
                    preview_url = f"https://drive.google.com/file/d/{file_id}/preview"
                    st.markdown(f"**ğŸ“„ {file_name}**")
                    st.components.v1.iframe(preview_url, height=400)

    except Exception as e:
        st.error(f"ğŸš¨ ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
