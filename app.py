import streamlit as st
import os
import tempfile
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.memory import ConversationBufferMemory
from google.oauth2 import service_account
from googleapiclient.discovery import build

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]

@st.cache_resource
def get_embeddings():
    return GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=st.secrets["GOOGLE_API_KEY"]
    )

@st.cache_resource
def init_drive_service():
    try:
        credentials = service_account.Credentials.from_service_account_info(
            st.secrets["google_credentials"],
            scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        return build('drive', 'v3', credentials=credentials, cache_discovery=False)
    except Exception as e:
        st.error(f"Drive ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return None

def get_pdf_files(service, folder_id):
    try:
        results = service.files().list(
            q=f"'{folder_id}' in parents and mimeType='application/pdf'",
            fields="files(id, name)"
        ).execute()
        return results.get('files', [])
    except Exception as e:
        st.error(f"Google Drive API ì˜¤ë¥˜: {str(e)}")
        return []

def process_pdf(pdf, service):
    try:
        request = service.files().get_media(fileId=pdf['id'])
        file_content = request.execute()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            temp_file.write(file_content)
            pdf_path = temp_file.name

        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        for doc in documents:
            doc.metadata["source"] = pdf["name"]

        os.unlink(pdf_path)
        return documents
    except Exception as e:
        st.warning(f"âš ï¸ {pdf['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

def create_vector_store(texts, embeddings):
    try:
        return FAISS.from_documents(texts, embeddings)
    except Exception as e:
        st.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def main():
    st.set_page_config(page_title="ê¸°ìˆ  ì†Œê°œ ì±—ë´‡", layout="wide")
    st.title("ğŸ’¡ ë³´ìœ  ê¸°ìˆ  ì•ˆë‚´ ì±—ë´‡")
    st.write("ìš°ë¦¬ íšŒì‚¬ê°€ ë³´ìœ í•œ ê¸°ìˆ  ì¤‘ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.")

    try:
        service = init_drive_service()
        embeddings = get_embeddings()

        if not service or not embeddings:
            st.stop()

        folder_id = st.secrets.get("FOLDER_ID")
        if not folder_id:
            st.error("ğŸ“‚ í´ë” IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        pdf_files = get_pdf_files(service, folder_id)
        if not pdf_files:
            st.warning("ğŸ“‚ PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        status_container = st.container()
        chat_container = st.container()

        if "analysis_completed" not in st.session_state:
            st.session_state.analysis_completed = False
            status_placeholder = status_container.empty()
            all_texts = []

            for idx, pdf in enumerate(pdf_files, 1):
                status_placeholder.info(f"ğŸ“„ ê¸°ìˆ  ë¬¸ì„œ ë¶„ì„ ì¤‘... ({idx}/{len(pdf_files)})\ní˜„ì¬: {pdf['name']}")
                documents = process_pdf(pdf, service)
                all_texts.extend(documents)

            status_placeholder.info("ğŸ§  í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
            split_texts = text_splitter.split_documents(all_texts)

            status_placeholder.info("ğŸ§  ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
            vector_store = create_vector_store(split_texts, embeddings)

            if not vector_store:
                return

            st.session_state.vector_store = vector_store
            st.session_state.all_pdfs = pdf_files
            st.session_state.analysis_completed = True

        if st.session_state.analysis_completed:
            with status_container:
                st.success("âœ… ê¸°ìˆ  ìë£Œ ë¶„ì„ ì™„ë£Œ! ê¶ê¸ˆí•œ ê¸°ìˆ ì„ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

            retriever = st.session_state.vector_store.as_retriever(search_kwargs={"k": 5})
            system_template = """
            ë„ˆëŠ” ìš°ë¦¬ íšŒì‚¬ê°€ ë³´ìœ í•œ ê¸°ìˆ  ê´€ë ¨ PDF ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ìì˜ ìš”ì²­ì— ì‘ë‹µí•˜ëŠ” AI ê¸°ìˆ  ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

            ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

            1. ì§ˆë¬¸ìê°€ ì–´ë–¤ ê¸°ìˆ ì— ê´€ì‹¬ì´ ìˆëŠ”ì§€ íŒŒì•…í•˜ì—¬, ê·¸ì™€ ê´€ë ¨ëœ **ìš°ë¦¬ íšŒì‚¬ê°€ ë³´ìœ í•œ ê¸°ìˆ **ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
            2. ê°€ëŠ¥í•œ ê²½ìš°, ê´€ë ¨ ê¸°ìˆ ì´ ì–¸ê¸‰ëœ **ë¬¸ì„œ ì´ë¦„ê³¼ í˜ì´ì§€ ë²ˆí˜¸**ë¥¼ í•¨ê»˜ ëª…ì‹œí•©ë‹ˆë‹¤.
            3. ë‹µë³€ì€ ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ë©°, **2~4ë¬¸ì¥ ì´ë‚´**ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
            4. ê´€ë ¨ëœ ê¸°ìˆ ì´ í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ë§í•©ë‹ˆë‹¤.
            5. ëª¨ë“  ë‹µë³€ì€ ì œê³µëœ ê¸°ìˆ ìë£Œ ë‚´ìš©ì— ê¸°ë°˜í•´ì•¼ í•˜ë©°, ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

            ê¸°ìˆ ìë£Œ ìš”ì•½:
            ----------------
            {context}
            """
            messages = [
                SystemMessagePromptTemplate.from_template(system_template),
                HumanMessagePromptTemplate.from_template("{question}")
            ]
            prompt = ChatPromptTemplate.from_messages(messages)

            if "memory" not in st.session_state:
                st.session_state.memory = ConversationBufferMemory(
                    memory_key="chat_history",
                    return_messages=True,
                    output_key="answer"
                )

            llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-pro",
                temperature=0.7,
                max_output_tokens=2048,
            )

            chain = ConversationalRetrievalChain.from_llm(
                llm=llm,
                retriever=retriever,
                memory=st.session_state.memory,
                combine_docs_chain_kwargs={"prompt": prompt},
                return_source_documents=True
            )

            if "messages" not in st.session_state:
                st.session_state.messages = []

            # ğŸ“ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
            if prompt := st.chat_input("ê¶ê¸ˆí•œ ê¸°ìˆ ì´ë‚˜ íŠ¹í—ˆì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”"):
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
                    response = chain({"question": prompt})
                    answer = response["answer"]

                    # ğŸ” ì¶œì²˜ ë¬¸ì„œ ì •ë¦¬
                    source_docs = response.get("source_documents", [])
                    sources = set()
                    for doc in source_docs:
                        filename = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ")
                        page = doc.metadata.get("page", "ì•Œ ìˆ˜ ì—†ëŠ” í˜ì´ì§€")
                        sources.add(f"- `{filename}`, í˜ì´ì§€ {page}")

                    # ğŸ”§ ë‹µë³€ì— ì¶œì²˜ ì¶”ê°€
                    if sources:
                        answer += "\n\n---\nğŸ“„ **ì°¸ê³  ë¬¸ì„œ:**\n" + "\n".join(sources)

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer
                    })

            # ğŸ’¬ ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
            for i in range(len(st.session_state.messages) - 1, -1, -2):
                if i > 0 and st.session_state.messages[i - 1]["role"] == "user":
                    st.markdown(f"**ğŸ™‹ ì§ˆë¬¸:** {st.session_state.messages[i - 1]['content']}")
                st.markdown(f"**ğŸ¤– ë‹µë³€:** {st.session_state.messages[i]['content']}")

            # ğŸ“š PDF ë¯¸ë¦¬ë³´ê¸°
            with st.expander("ğŸ“‚ PDF ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                for pdf in st.session_state.all_pdfs:
                    file_id = pdf["id"]
                    file_name = pdf["name"]
                    preview_url = f"https://drive.google.com/file/d/{file_id}/preview"
                    st.markdown(f"**ğŸ“„ {file_name}**")
                    st.components.v1.iframe(preview_url, height=400)

    except Exception as e:
        st.error(f"ğŸš¨ ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
