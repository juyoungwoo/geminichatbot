import streamlit as st
import os
import tempfile
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter 
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.memory import ConversationBufferMemory
from google.oauth2 import service_account
from googleapiclient.discovery import build

# âœ… Gemini API í‚¤ ì„¤ì •
os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]

# âœ… ì„ë² ë”© ëª¨ë¸ ìºì‹± (ë©”ëª¨ë¦¬ ì ˆì•½)
@st.cache_resource
def get_embeddings():
    return GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",  # Googleì˜ ì„ë² ë”© ëª¨ë¸
        google_api_key=st.secrets["GOOGLE_API_KEY"]  # ê¸°ì¡´ì— ì„¤ì •í•œ API í‚¤ ì‚¬ìš©
    )

# âœ… Google Drive API ì´ˆê¸°í™”
@st.cache_resource
def init_drive_service():
    credentials = service_account.Credentials.from_service_account_info(
        st.secrets["google_credentials"],
        scopes=['https://www.googleapis.com/auth/drive.readonly']
    )
    return build('drive', 'v3', credentials=credentials)

# âœ… PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_pdf_files(service, folder_id):
    try:
        results = service.files().list(
            q=f"'{folder_id}' in parents and mimeType='application/pdf'",
            fields="files(id, name)"
        ).execute()
        return results.get('files', [])
    except Exception as e:
        st.error(f"Google Drive API ì˜¤ë¥˜: {str(e)}")
        return []

# âœ… Streamlit UI ì‹œì‘
st.title("ğŸ“„ IPRì‹¤ ë§¤ë‰´ì–¼ AI ì±—ë´‡")
st.write("â˜† ìë£Œ ìˆ˜ì • ë˜ëŠ” ì¶”ê°€ í¬ë§ì‹œ ì£¼ì˜ ì—°êµ¬ì› ì—°ë½ â˜†")

try:
    # âœ… Google Driveì—ì„œ PDF ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    service = init_drive_service()
    FOLDER_ID = '1fThzSsDTeZA6Zs1VLGNPp6PejJJVydra'
    pdf_files = get_pdf_files(service, FOLDER_ID)

    if not pdf_files:
        st.warning("ğŸ“‚ ë§¤ë‰´ì–¼ í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info(f"ğŸ“„ ì´ {len(pdf_files)}ê°œì˜ ë§¤ë‰´ì–¼ì„ ë¶„ì„ ì¤‘...")

        # âœ… ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬ ë° ì„ë² ë”© ë²¡í„° ì €ì¥
        @st.cache_resource
        def process_all_pdfs():
            all_texts = []

            for pdf in pdf_files:
                try:
                    request = service.files().get_media(fileId=pdf['id'])
                    file_content = request.execute()

                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                        temp_file.write(file_content)
                        pdf_path = temp_file.name

                    loader = PyPDFLoader(pdf_path)
                    documents = loader.load()

                    for doc in documents:
                        doc.metadata['source'] = pdf['name']

                    all_texts.extend(documents)

                    os.unlink(pdf_path)  # ì‚¬ìš© í›„ íŒŒì¼ ì‚­ì œ

                except Exception as e:
                    st.warning(f"âš ï¸ {pdf['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

            # âœ… ë¬¸ì„œ ë¶„í•  ìµœì í™”
            text_splitter = ecursiveCharacterTextSplitter(
                chunk_size=1000,  
                chunk_overlap=100,  
                length_function=len,
                separators=["\n\n", "\n", " ", ""],
                is_separator_regex=False
            )
            split_texts = text_splitter.split_documents(all_texts)

            # âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ìºì‹±
            embeddings = get_embeddings()
            vector_store = FAISS.from_documents(split_texts, embeddings)

            return vector_store

        # âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vector_store = process_all_pdfs()
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})  # ê²€ìƒ‰ ê²°ê³¼ ìµœì í™”

        # âœ… AI í”„ë¡¬í”„íŠ¸ ì„¤ì •
        system_template = """
        ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        ë‹µì„ ëª¨ë¥´ëŠ” ê²½ìš° "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ê³ , ë‹µë³€ì„ ë§Œë“¤ì–´ë‚´ë ¤ í•˜ì§€ ë§ˆì„¸ìš”.
        ê°€ëŠ¥í•œ ê²½ìš° ì •ë³´ì˜ ì¶œì²˜(ë¬¸ì„œ)ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
        ----------------
        {context}
        """
        messages = [
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template("{question}")
        ]
        prompt = ChatPromptTemplate.from_messages(messages)

        # âœ… ë©”ëª¨ë¦¬ ì„¤ì •
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer",  # ì¶œë ¥ í‚¤ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
            verbose=False  # ì¶”ê°€
        )

        # âœ… LLM ëª¨ë¸ ì„¤ì •
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash-8b",  # ìˆ˜ì •ëœ ëª¨ë¸ëª…
            temperature=0.3,
            max_output_tokens=2048,
        )


        # âœ… ëŒ€í™” ì²´ì¸ ì„¤ì •
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            combine_docs_chain_kwargs={'prompt': prompt},
            return_source_documents=True
        )

        # âœ… ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("ğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤– ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    response = chain({"question": prompt})
                    st.markdown(response['answer'])
                    
                    # ì°¸ê³ í•œ ë¬¸ì„œ ì¶œì²˜ í‘œì‹œ
                    sources = set([doc.metadata['source'] for doc in response['source_documents']])
                    if sources:
                        st.markdown("---")
                        st.markdown("**ì°¸ê³ í•œ ë¬¸ì„œ:**")
                        for source in sources:
                            st.markdown(f"- {source}")
                            
                    st.session_state.messages.append({"role": "assistant", "content": response['answer']})

except Exception as e:
    st.error(f"ğŸš¨ ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
