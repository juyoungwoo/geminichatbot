import streamlit as st
import os
import io
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts.chat import (
   ChatPromptTemplate,
   SystemMessagePromptTemplate,
   HumanMessagePromptTemplate,
)
from google.oauth2 import service_account
from googleapiclient.discovery import build
import tempfile

st.write("ğŸ” Checking secrets...")

if "GOOGLE_API_KEY" not in st.secrets:
    st.error("ğŸš¨ `GOOGLE_API_KEY`ê°€ `secrets.toml`ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
else:
    st.success("âœ… `GOOGLE_API_KEY` ë¡œë“œ ì„±ê³µ!")

if "google_credentials" not in st.secrets:
    st.error("ğŸš¨ `google_credentials`ê°€ `secrets.toml`ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
else:
    st.success("âœ… `google_credentials` ë¡œë“œ ì„±ê³µ!")

# Google Gemini API í‚¤ ì„¤ì •
os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]

# ì„ë² ë”© ëª¨ë¸ ìºì‹±
@st.cache_resource
def get_embeddings():
   return GoogleGenerativeAIEmbeddings(model="text-embedding-004")

# Google Drive ì„¤ì •
@st.cache_resource
def init_drive_service():
   credentials = service_account.Credentials.from_service_account_info(
       st.secrets["google_credentials"],
       scopes=['https://www.googleapis.com/auth/drive.readonly']
   )
   return build('drive', 'v3', credentials=credentials)

# PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_pdf_files(service, folder_id):
   results = service.files().list(
       q=f"'{folder_id}' in parents and mimeType='application/pdf'",
       fields="files(id, name)"
   ).execute()
   return results.get('files', [])

# Streamlit UI êµ¬ì„±
st.title("ğŸ“„ IPRì‹¤ ë§¤ë‰´ì–¼ AI ì±—ë´‡")
st.write("â˜† ìë£Œ ìˆ˜ì • ë˜ëŠ” ì¶”ê°€ í¬ë§ì‹œ ì£¼ì˜ ì—°êµ¬ì› ì—°ë½ â˜†")

try:
   # ë“œë¼ì´ë¸Œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
   service = init_drive_service()
   FOLDER_ID = '1fThzSsDTeZA6Zs1VLGNPp6PejJJVydra'

   # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
   pdf_files = get_pdf_files(service, FOLDER_ID)

   if not pdf_files:
       st.warning("í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
   else:
       st.info(f"ì´ {len(pdf_files)}ê°œì˜ ë§¤ë‰´ì–¼ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

       # ëª¨ë“  PDF íŒŒì¼ì˜ ë‚´ìš©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
       @st.cache_resource
       def process_all_pdfs():
           all_texts = []
           for pdf in pdf_files:
               try:
                   request = service.files().get_media(fileId=pdf['id'])
                   file_content = request.execute()

                   with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                       temp_file.write(file_content)
                       pdf_path = temp_file.name

                   loader = PyPDFLoader(pdf_path)
                   documents = loader.load()

                   for doc in documents:
                       doc.metadata['source'] = pdf['name']

                   all_texts.extend(documents)
                   os.unlink(pdf_path)

               except Exception as e:
                   st.warning(f"{pdf['name']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

           # ë¬¸ì„œ ë¶„í•  ìµœì í™”
           text_splitter = CharacterTextSplitter(
               chunk_size=500,  
               chunk_overlap=200,
               separator="\n"
           )
           split_texts = text_splitter.split_documents(all_texts)

           # ìºì‹œëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
           embeddings = get_embeddings()
           vector_store = FAISS.from_documents(split_texts, embeddings)

           return vector_store

       # ëª¨ë“  PDF ì²˜ë¦¬
       vector_store = process_all_pdfs()
       retriever = vector_store.as_retriever(search_kwargs={"k": 3})

       # í”„ë¡¬í”„íŠ¸ ì„¤ì •
       system_template = """
       Use the following pieces of context to answer the users question shortly.
       Given the following summaries of a long document and a question.
       If you don't know the answer, just say that "I don't know", don't try to make up an answer.
       If possible, mention which document (source) the information comes from.
       ----------------
       {summaries}
       You MUST answer in Korean and in Markdown format:
       """
       messages = [
           SystemMessagePromptTemplate.from_template(system_template),
           HumanMessagePromptTemplate.from_template("{question}")
       ]
       prompt = ChatPromptTemplate.from_messages(messages)
       chain_type_kwargs = {"prompt": prompt}

       # Google Gemini LLM ì„¤ì •
       llm = ChatGoogleGenerativeAI(
           model="gemini-1.5-flash",
           temperature=0,
           google_api_key=os.environ["GOOGLE_API_KEY"],
           max_tokens=512
       )

       # Conversational QA ì²´ì¸ ì„¤ì •
       chain = ConversationalRetrievalChain.from_llm(
           llm=llm,
           retriever=retriever,
           return_source_documents=True,
           chain_type_kwargs=chain_type_kwargs
       )

       # ëŒ€í™” ê¸°ë¡ ì €ì¥
       if "chat_history" not in st.session_state:
           st.session_state.chat_history = []

       # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
       query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
       if st.button("ì§ˆë¬¸í•˜ê¸°") and query:
           with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
               result = chain({"question": query, "chat_history": st.session_state.chat_history}, return_only_outputs=True)
               answer = result["answer"]
               st.session_state.chat_history.append((query, answer))
               st.markdown(answer)

except Exception as e:
   st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
